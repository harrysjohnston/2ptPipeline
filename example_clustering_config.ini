[DEFAULT]
root=

; any argument from these .ini files can be overridden on the command-line
; with syntax -p <sect1>.<arg1>=X <sect1>.<arg2>=Y <sect2>.<arg3>=Z
; e.g. -p jackknife.run=1 wgplus_config.min_rpar=0
; etc.

[treecorr_config]
; can overwrite any treecorr parameters in this section
;
; specify defaults-file if wanted
default=/default/treecorr/config/file

[jackknife]
; only this section is fed to skyknife.py
;
; randoms fits-catalogue for jackknife construction
catalog=%(root)s/PAU_BCNZ_W3KSB_CloneZIDRandoms.fits
; transfer jackknife columns into each specified catalog below
; syntax = path ra_name dec_name z_name
; remember // at end of line and ensure there are no other // in the paths
exportto=
	/share/splinter/hj/PhD/PAU/PAU_BCNZ_W3KSB.fits ra dec zb //
	/share/splinter/hj/PhD/PAU/PAU_BCNZ_W3KSB_red.fits ra dec zb //
	/share/splinter/hj/PhD/PAU/PAU_BCNZ_W3KSB_blue.fits ra dec zb
; 0=no jackknife, 1=compute jackknife only, 2=jackknife before main, 3=jackknife after main, 4=collect jackknife only
run=0
; specify jackknife indices to be measured (as space-separated integers)
numbers=
; fits column names for 'catalog' above
ra_col=ra
dec_col=dec
z_col=z
r_col=comoving
; optionally give RA above which to subtract 360 -- set to 360 for no action
; nicer plots and may aid with quartering
shiftra=360
; minimum angular scale of jackknife regions [deg]
minimum_scale=0.5
; tolerance on area when dividing regions -- increase for more tolerance (in[0,1])
quarter_tol=0.15
; tolerance on minimum_scale when dividing regions -- decrease for more tolerance (in[0,1])
scale_tol=0.7
; 0=2D jackknife, 1=3D jackknife
do_3d=0
; minimum radial length-scale for jackknife subregions
minimum_depth=150.
; redshift limits (if doing 3d), space-separated; zmin zmax
zlims=0.1 0.8
; 1 = plot skyknife and save as <catalog-name>.jk.png (replacing .fits)
plot=%(root)s/plot.png

[wgplus_config]
; this section specifies options for projected statistics (wgg, wgp)
;
; only FITS supported so far
file_type=FITS
; PW1 for standard IA estimator, AS for average shear estimator
estimator=PW1
; fits column names
ra_col=alpha_j2000
dec_col=delta_j2000
r_col=comoving_mpc_zb
g1_col=e1ok
g2_col=e2ok
rand_ra_col=ra
rand_dec_col=dec
rand_r_col=comoving_mpc_z
; degrees | radians | arcmin
ra_units=degrees
dec_units=degrees
; 1=flip, 0=do not flip
flip_g1=1
flip_g2=0
; metrics other than Rperp not currently implemented -- see Treecorr docs for more
metric=Rperp
; bin-mixing approximation, should be <0.05 for science (<0.02 safer)
; set higher for fast computation, but biased large scales
bin_slop=0.
; transverse separation limits in units of r_col
min_sep=0.05
max_sep=20.
; transverse bins
nbins=10
; line of sight separation limits in units of r_col
min_rpar=-150.
max_rpar=150.
; line of sight bins
nbins_rpar=50
; specify Pi-bin edges, comma-separated -- this will overwrite min/max_rpar and nbins_rpar
rpar_edges=
; 1=do randoms-subtracted statistics
compensated=1
; 1=compute only pairs with LoS separation in abs[max_rpar, 1.5*max_rpar]
largePi=0
; 1=save out 2D correlation functions as pickle file
save_3d=0

[catalogs]
; directories for data/randoms, will be prefixed to data/rand paths
ddir=%(root)s
rdir=%(root)s
; the rest of this section specifies N correlations to be performed
; each line is a new correlation, and should have // at the end
; all arguments should have either:
; 	- one single line (to be applied to all correlations)
;	- N lines (to specify arguments for each correlation)
; be sure to indent multi-line arguments ('//' interpreted as a line-break)
; data1/rand1 will be copied into data2/rand2 if those are empty
;
; 'wgg'=proj. clustering, 'wgp'=proj. IA, 'wth'=angular clustering
corr_types=
	wgg
; data/randoms filenames
data1=
	data.fits
data2=
rand1=
	randoms.fits
rand2=
; cuts to be applied to corresponding catalogues
; syntax is python, using fits column-names
; e.g. (zb > 0.2) finds column 'zb' and applies the greater-than 0.2 cut
; e.g. 'edge_cut' is a boolean column to be applied as a cut
; e.g. could reverse this/any cut with '~edge_cut'
; some in-built functions available -- see top of w_pipeline.py
data_cuts1=
	(zb > 0.2) & edge_cut & hqcut(qz, 10.) //
	(zb > 0.2) & edge_cut & hqcut(qz, 20.) //
	(zb > 0.2) & edge_cut & hqcut(qz, 30.) //
	(zb > 0.2) & edge_cut & hqcut(qz, 40.) //
	(zb > 0.2) & edge_cut & hqcut(qz, 50.) //
	(zb > 0.2) & edge_cut & hqcut(qz, 60.) //
	(zb > 0.2) & edge_cut & hqcut(qz, 70.) //
	(zb > 0.2) & edge_cut & hqcut(qz, 80.) //
	(zb > 0.2) & edge_cut & hqcut(qz, 90.) //
	(zb > 0.2) & edge_cut
data_cuts2=
rand_cuts1=
	(z > 0.2) & edge_cut
rand_cuts2=
; weights per galaxy, per catalogue, python syntax as with cuts
data_weights1=
	(1. / qz)
data_weights2=
rand_weights1=
rand_weights2=

[output]
; this section just specifies output paths
;
; output directory name
savedir=%(root)s/OUTPUTS
; outfilename per correlation -- will create default names if left blank
out_corrs=
	wgg_wqz_qzltperc10 //
	wgg_wqz_qzltperc20 //
	wgg_wqz_qzltperc30 //
	wgg_wqz_qzltperc40 //
	wgg_wqz_qzltperc50 //
	wgg_wqz_qzltperc60 //
	wgg_wqz_qzltperc70 //
	wgg_wqz_qzltperc80 //
	wgg_wqz_qzltperc90 //
	wgg_wqz_qzltperc100

; ignore
[runtime]
#serial=T



