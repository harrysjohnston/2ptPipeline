[DEFAULT]
; config file variables, used as e.g. %(root)s/
; can define these variables in any section
; arguments can continue over multiple lines with indentation
; and may require // at line-end - see notes
root=

[jackknife]
; options for skyknife.py jackknife sample maker, using iterative quartering or kmeans clustering (command-line arg for latter)
; creates column in catalogues 'jackknife_ID' - can use w_pipeline.py jackknife functionality with
; own jackknife_ID column if desired, but ensure that it is base-1, as jackknife_ID==0 are always discarded
; catalogue to define jackknife
catalog=test_rand.fits
; additional catalogues for export of the same jackknife regions
exportto=
	test_data.fits ra dec z
; w_pipeline.py option: measure 1=jackknife correlations only, 2=jackknife then main, 3=main then jackknife, 4=collect jackknife covariance, 0=main-only
run=3
; can specify limited subset (space-separated) of jackknife indices to correlate
numbers=
; 'catalog' columns for jackknife definition
ra_col=ra
dec_col=dec
z_col=z
; required for 3D jackknife
r_col=chi
; optionally subtract 360 from RA beyond this point
; recommended for RA=zero-crossing footprints
shiftra=360
; jackknife tuning parameters: target smallest degree scale
minimum_scale=0.5
; tolerance of area disparities, increase to <=1 for more tolerance
quarter_tol=0.1
; tolerance of angular size disparities, decrease to >=0 for more tolerance
scale_tol=0.8
; flag to slice jackknife regions into equal-number redshift subsets
do_3d=1
; minimum depth of slices, in units of 'r_col'
minimum_depth=150.
; min and max redshift of jackknife regions
zlims=0.1 0.3
; flag to make a plot of the jackknife and save as <catalog name without FITS extension>.jk.png
plot=1

[angular_correlations]
; give a path to a treecorr config
default=
; or just specify the options here
; only FITS supported
file_type=FITS
; bin_slop can be overrided with command-line arg
bin_slop=0.01
; for angular correlations only:
ra_col=ra
dec_col=dec
ra_units=degrees
dec_units=degrees
nbins=30
min_sep=3.
max_sep=300.
sep_units=arcmin
; compensated = LS estimator
nn_statistic=compensated
; command-line args and defaults override num_threads
; and verbosity, so do not bother defining in config
;num_threads=
;verbose=

[projected_correlations]
; only FITS supported
file_type=FITS
; AS=average shear/PW1=pair-weighted(fast)/PW2=pair-weighted(slow)
; determines normalisation, by DDs(AS), RDs(PW1), or RdRs(PW2)
gplus_estimator=PW1
; data catalogue column-names (must be the same for all data catalogues in this config)
; spherical
ra_col=ra
dec_col=dec
r_col=chi
ra_units=degrees
dec_units=degrees
; or Cartesian
x_col=
y_col=
z_col=
; shear1 and 2
g1_col=e1
g2_col=e2
; randoms catalogue column-names (must be the same for all randoms catalogues in this config)
rand_ra_col=
rand_dec_col=
rand_r_col=
rand_x_col=
rand_y_col=
rand_z_col=
; flag to multiply shear1 and/or 2 by -1
flip_g1=0
flip_g2=0
; supported are 'Periodic' for box correlations
; and 'Rperp' for projected RA/Dec/r correlations
metric=Rperp
; box size in units of 'z_col', if doing Periodic
period=
; bin_slop here can be overriden with the command-line arg
bin_slop=0.
; basic binning
nbins=7
min_sep=0.1
max_sep=30.
; line-of-sight binning (Rperp), either limits and nbins
min_rpar=-40
max_rpar=40
nbins_rpar=20
; or give specific edges, if wanting irregularity
rpar_edges=
; compensated = randoms-subtracted
compensated=1
; perform large line-of-sight separation tests, with 1.5*max_rpar (Rperp)
largePi=0
; save-out xi(rp, Pi) in pickle dicts <outfile>.p
save_3d=0

[catalogs]
; for every argument in this section, each row is an individual
; correlation, and must be //-separated. Python broadcasting
; rules apply: for given N correlations, all arguments should
; have N rows detailing args for each correlation, or 1 row
; to be carried over to all correlations. If data2/rand2 are blank,
; they take the values of data1/rand1, respectively.
; trailing // should not be a problem

; wgg/wgp/wth = proj.clust/proj.IA/ang.clust
; recommend proj/ang to have separate config files
; for wgp, data1=density, data2=shapes
corr_types=
	wgg //
	wgp
data1=
	test_data.fits
data2=
rand1=
	test_rand.fits
rand2=
; python syntax for cuts, using names of FITS catalogue columns
; special cut 'downsample(f)' randomly downsamples to fraction f
; can include other functions at the top of w_pipeline.py for making
; cuts per correlation, e.g. 'idmatch(r1, d1, r_ID, d_ID)' will
; select rand1 objects which have r_ID existing in data1 column d_ID
; (first 2 args must be in {r1, r2, d1, d2}, second 2 are FITS column-names)
data_cuts1=
; 'none' can be a place-holder for no cuts
data_cuts2=
rand_cuts1=
rand_cuts2=
; similarly 'ones' signifies unit-weighting
; no syntax here, only FITS column-names; weights should be predefined
data_weights1=
data_weights2=
rand_weights1=
rand_weights2=

[output]
savedir=./OUTPUTS/
; outfile names; row per correlation, otherwise default names will be given
; will be appended with .dat, if not already
out_corrs=
	wgg //
	wgp



